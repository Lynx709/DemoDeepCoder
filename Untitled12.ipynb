{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled12.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyNRkcmi4E9xodKBh+dqAPy4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Lynx709/auto_codeforce/blob/main/Untitled12.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MrdJVKjykbAD"
      },
      "outputs": [],
      "source": [
        "    %matplotlib inline"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import subprocess\n",
        "import zipfile\n",
        "\n",
        "if os.path.exists('./data'):\n",
        "  print(\"./dat/ already exists\")\n",
        "else:\n",
        "  subprocess.run(\"wget https://download.pytorch.org/tutorial/data.zip\",shell=True, check=True)\n",
        "  with zipfile.ZipFile(\"./data.zip\") as zipfile:\n",
        "    zipfile.extractall(\".\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JLSdxrnbllkW",
        "outputId": "3e296be7-3b47-4e30-e086-891c68d78341"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "./dat/ already exists\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from __future__ import unicode_literals, print_function, division\n",
        "from io import open\n",
        "import glob\n",
        "import os\n",
        "import unicodedata\n",
        "import string\n",
        "\n",
        "all_letters = string.ascii_letters + \" .,;'-\"\n",
        "n_letters = len(all_letters) + 1 # Plus EOS marker\n",
        "\n",
        "def findFiles(path): return glob.glob(path)\n",
        "\n",
        "# Turn a Unicode string to plain ASCII, thanks to https://stackoverflow.com/a/518232/2809427\n",
        "def unicodeToAscii(s):\n",
        "    return ''.join(\n",
        "        c for c in unicodedata.normalize('NFD', s)\n",
        "        if unicodedata.category(c) != 'Mn'\n",
        "        and c in all_letters\n",
        "    )\n",
        "\n",
        "# Read a file and split into lines\n",
        "def readLines(filename):\n",
        "    lines = open(filename, encoding='utf-8').read().strip().split('\\n')\n",
        "    return [unicodeToAscii(line) for line in lines]\n",
        "\n",
        "# Build the category_lines dictionary, a list of lines per category\n",
        "category_lines = {}\n",
        "all_categories = []\n",
        "for filename in findFiles('data/names/*.txt'):\n",
        "    category = os.path.splitext(os.path.basename(filename))[0]\n",
        "    all_categories.append(category)\n",
        "    lines = readLines(filename)\n",
        "    category_lines[category] = lines\n",
        "\n",
        "n_categories = len(all_categories)\n",
        "\n",
        "if n_categories == 0:\n",
        "    raise RuntimeError('Data not found. Make sure that you downloaded data '\n",
        "        'from https://download.pytorch.org/tutorial/data.zip and extract it to '\n",
        "        'the current directory.')\n",
        "\n",
        "print('# categories:', n_categories, all_categories)\n",
        "print(unicodeToAscii(\"O'Néàl\"))"
      ],
      "metadata": {
        "id": "uR47TdGsndfy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 新しいセクション\n",
        "\n",
        "修正後のコード全体が出てくる\n",
        "- 文に区切る　.とか空白改行で切る\n",
        "- eng　(tab) fraが必要\n",
        "- 左修正前 (tab)　右修正後のコード　=> 全く変わらないこともある(左右で相違なし)\n",
        "- (ここがハード)文脈を考慮するモデルでないといけない=>こうしないとまともに動かない。\n",
        "\n",
        "# 先にやるのはこっち\n",
        " 間違った箇所だけを指摘する。　＝＞　エラーの原因を表示する\n",
        "- 分類問題に近い\n",
        "- トークンに区切る　=>　間違いかどうかのタグをつける(OかXか)\n",
        "- BOI タグ方式　=> o;out(間違いではない), B:beginココから間違い　I: inまだ間違い中\n",
        "- 品詞タグ付け\n",
        "- 元々のコード(トークンに区切ったもの)とタグの列\n"
      ],
      "metadata": {
        "id": "ySjEGHFoE6CZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install sentencepiece"
      ],
      "metadata": {
        "id": "sqRnTmDDE_Ef",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3555a05b-36fe-4579-e3a2-40cee3cc71ba"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting sentencepiece\n",
            "  Downloading sentencepiece-0.1.96-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.2 MB 27.1 MB/s \n",
            "\u001b[?25hInstalling collected packages: sentencepiece\n",
            "Successfully installed sentencepiece-0.1.96\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KoQTVmjbvUFu",
        "outputId": "07d5f141-47bb-405b-8634-269a0759cc5a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd \"drive/My Drive/Colab Notebooks\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PjYgjj93vqZP",
        "outputId": "e44d44a4-fa49-4281-e445-49692f4aeb61"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Errno 2] No such file or directory: 'drive/My Drive/Colab Notebooks'\n",
            "/content/drive/My Drive/Colab Notebooks\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import codecs\n",
        "import re\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "\n",
        "corpus = []\n",
        "with open('./c.txt') as f:\n",
        "    lines = f.read()\n",
        "    \n",
        "    for l in lines.split(\"\\n\"):\n",
        "      l = l.lstrip()\n",
        "      corpus.append(l)\n",
        "\n",
        "# ファイルの保存       \n",
        "print(*corpus, sep=\"\\n\", file=codecs.open(\"cpp.txt\", \"w\", \"utf-8\"))\n",
        "\n",
        "##with open('./cpp.txt') as f:\n",
        "  ##  for line in f:\n",
        "    ##   print(line)"
      ],
      "metadata": {
        "id": "UXPGhIQ2xpRI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import sentencepiece as spm\n",
        "\n",
        "# 学習の実行\n",
        "spm.SentencePieceTrainer.Train(\n",
        "   input=\"cpp.txt\",\n",
        "   model_prefix=\"sentencepiece\",\n",
        "   model_type=\"word\",\n",
        "   vocab_size=91,\n",
        "   add_dummy_prefix=\"false\", #先頭の_を入れるか入れないか => 先頭に来る単語と文中に来る単語で意味が異なる可能性によってtrue,　falseをきめる\n",
        ")"
      ],
      "metadata": {
        "id": "62ZsY9Agvr9e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import sentencepiece as spm\n",
        "\n",
        "# モデルの作成\n",
        "sp = spm.SentencePieceProcessor()\n",
        "sp.Load(\"sentencepiece.model\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hNNYz0u-wSZC",
        "outputId": "1d88ebfe-788b-42e1-ac47-816d897ebb4b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# テキストを語彙列に分割\n",
        "for i in corpus:\n",
        "  print(i)\n",
        "  print(sp.EncodeAsPieces(i))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hB6i1KmMwV4d",
        "outputId": "c90a1542-ddaa-48dc-af53-84eb763a5f26"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "#include <iostream>\n",
            "['#include', '▁<iostream>']\n",
            "#include <fstream>\n",
            "['#include', '▁<fstream>']\n",
            "#include <Eigen/Core>\n",
            "['#include', '▁<Eigen/Core>']\n",
            "using namespace Eigen;\n",
            "['using', '▁namespace', '▁Eigen;']\n",
            "\n",
            "[]\n",
            "Vector3f RungeKutta(float t, Vector3f X);\n",
            "['Vector3f', '▁RungeKutta(float', '▁t,', '▁Vector3f', '▁X);']\n",
            "Vector3f LorentzEquation(float t, Vector3f X);\n",
            "['Vector3f', '▁LorentzEquation(float', '▁t,', '▁Vector3f', '▁X);']\n",
            "\n",
            "[]\n",
            "const float p = 10;\n",
            "['const', '▁float', '▁p', '▁=', '▁10;']\n",
            "const float r = 28;\n",
            "['const', '▁float', '▁r', '▁=', '▁28;']\n",
            "const float b = 8/3;\n",
            "['const', '▁float', '▁b', '▁=', '▁8/3;']\n",
            "const float dt = 0.01;\n",
            "['const', '▁float', '▁dt', '▁=', '▁0.01;']\n",
            "const float t_0 = 0;\n",
            "['const', '▁float', '▁t_0', '▁=', '▁0;']\n",
            "const float t_1 = 10;\n",
            "['const', '▁float', '▁t_1', '▁=', '▁10;']\n",
            "const Vector3f X_0(1, 1, 1);\n",
            "['const', '▁Vector3f', '▁X_0(1,', '▁1,', '▁1);']\n",
            "\n",
            "[]\n",
            "int main() {\n",
            "['int', '▁main()', '▁{']\n",
            "float t = t_0; \n",
            "['float', '▁t', '▁=', '▁t_0;']\n",
            "Vector3f X = X_0;\n",
            "['Vector3f', '▁X', '▁=', '▁X_0;']\n",
            "\n",
            "[]\n",
            "std::ofstream ofs;\n",
            "['std::ofstream', '▁ofs;']\n",
            "ofs.open(\"lorentz_attractor.txt\", std::ios::out);\n",
            "['ofs.open(\"lorentz_attractor.txt\",', '▁std::ios::out);']\n",
            "ofs << t << \" \" << X.transpose() << std::endl; \n",
            "['ofs', '▁<<', '▁t', '▁<<', '▁\"', '▁\"', '▁<<', '▁X.transpose()', '▁<<', '▁std::endl;']\n",
            "\n",
            "[]\n",
            "do {\n",
            "['do', '▁{']\n",
            "X = RungeKutta(t, X);\n",
            "['X', '▁=', '▁RungeKutta(t,', '▁X);']\n",
            "t += dt;\n",
            "['t', '▁+=', '▁dt;']\n",
            "ofs << t << \" \" << X.transpose() << std::endl; \n",
            "['ofs', '▁<<', '▁t', '▁<<', '▁\"', '▁\"', '▁<<', '▁X.transpose()', '▁<<', '▁std::endl;']\n",
            "} while(t < t_1);\n",
            "['}', '▁while(t', '▁<', '▁t_1);']\n",
            "\n",
            "[]\n",
            "return 0; \n",
            "['return', '▁0;']\n",
            "}\n",
            "['}']\n",
            "\n",
            "[]\n",
            "Vector3f RungeKutta(float t, Vector3f X) {\n",
            "['Vector3f', '▁RungeKutta(float', '▁t,', '▁Vector3f', '▁X)', '▁{']\n",
            "Vector3f k_1 = LorentzEquation(t, X);\n",
            "['Vector3f', '▁k_1', '▁=', '▁LorentzEquation(t,', '▁X);']\n",
            "Vector3f k_2 = LorentzEquation(t + dt/2, X + dt/2*k_1);\n",
            "['Vector3f', '▁k_2', '▁=', '▁LorentzEquation(t', '▁+', '▁dt/2,', '▁X', '▁+', '▁dt/2*k_1);']\n",
            "Vector3f k_3 = LorentzEquation(t + dt/2, X + dt/2*k_2);\n",
            "['Vector3f', '▁k_3', '▁=', '▁LorentzEquation(t', '▁+', '▁dt/2,', '▁X', '▁+', '▁dt/2*k_2);']\n",
            "Vector3f k_4 = LorentzEquation(t + dt, X + dt*k_3);\n",
            "['Vector3f', '▁k_4', '▁=', '▁LorentzEquation(t', '▁+', '▁dt,', '▁X', '▁+', '▁dt*k_3);']\n",
            "Vector3f X_next = X + dt/6*(k_1 + 2*k_2 + 2*k_3 + k_4);\n",
            "['Vector3f', '▁X_next', '▁=', '▁X', '▁+', '▁dt/6*(k_1', '▁+', '▁2*k_2', '▁+', '▁2*k_3', '▁+', '▁k_4);']\n",
            "\n",
            "[]\n",
            "return X_next;\n",
            "['return', '▁X_next;']\n",
            "}\n",
            "['}']\n",
            "Vector3f LorentzEquation(float t, Vector3f X) { \n",
            "['Vector3f', '▁LorentzEquation(float', '▁t,', '▁Vector3f', '▁X)', '▁{']\n",
            "float x = X[0];\n",
            "['float', '▁x', '▁=', '▁X[0];']\n",
            "float y = X[1];\n",
            "['float', '▁y', '▁=', '▁X[1];']\n",
            "float z = X[2];\n",
            "['float', '▁z', '▁=', '▁X[2];']\n",
            "\n",
            "[]\n",
            "Vector3f val(-p*x + p*y, -x*z + r*x - y, x*y - b*z);\n",
            "['Vector3f', '▁val(-p*x', '▁+', '▁p*y,', '▁-x*z', '▁+', '▁r*x', '▁-', '▁y,▁x*y', '▁-', '▁b*z);']\n",
            "\n",
            "[]\n",
            "return val;  \n",
            "['return', '▁val;']\n",
            "}\n",
            "['}']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(sp.EncodeAsIds(corpus[0]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kAcfevoBwYAx",
        "outputId": "957ac9fd-8cf8-40ed-e9f6-462e2354df3a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[17, 50]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(sp.encode(corpus[0], out_type=str))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3y9UyGphB0Yj",
        "outputId": "610da73b-fad3-406c-ed9a-7fc955ee9638"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['#include', '▁<iostream>']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        ""
      ],
      "metadata": {
        "id": "dBlzj_MCB8hW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        ""
      ],
      "metadata": {
        "id": "yPXF1n44Bynk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import xml.etree.ElementTree as ET\n",
        "\n",
        "for i, token in enumerate(ET.parse('./nlp.txt.xml').iter('token')):\n",
        "    print('{}\\t{}\\t{}\\t{}'.format(i, token.findtext('word'), \n",
        "                                  token.findtext('lemma'), token.findtext('POS')))\n",
        "\n",
        "    # 多いので制限\n",
        "    if i > 30:\n",
        "        break"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VnK1CdUTsKKZ",
        "outputId": "eebd85b9-c3c0-40aa-a012-917b6e17afad"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0\tNatural\tnatural\tJJ\n",
            "1\tlanguage\tlanguage\tNN\n",
            "2\tprocessing\tprocessing\tNN\n",
            "3\tFrom\tfrom\tIN\n",
            "4\tWikipedia\tWikipedia\tNNP\n",
            "5\t,\t,\t,\n",
            "6\tthe\tthe\tDT\n",
            "7\tfree\tfree\tJJ\n",
            "8\tencyclopedia\tencyclopedia\tNN\n",
            "9\tNatural\tnatural\tJJ\n",
            "10\tlanguage\tlanguage\tNN\n",
            "11\tprocessing\tprocessing\tNN\n",
            "12\t-LRB-\t-lrb-\t-LRB-\n",
            "13\tNLP\tnlp\tNN\n",
            "14\t-RRB-\t-rrb-\t-RRB-\n",
            "15\tis\tbe\tVBZ\n",
            "16\ta\ta\tDT\n",
            "17\tfield\tfield\tNN\n",
            "18\tof\tof\tIN\n",
            "19\tcomputer\tcomputer\tNN\n",
            "20\tscience\tscience\tNN\n",
            "21\t,\t,\t,\n",
            "22\tartificial\tartificial\tJJ\n",
            "23\tintelligence\tintelligence\tNN\n",
            "24\t,\t,\t,\n",
            "25\tand\tand\tCC\n",
            "26\tlinguistics\tlinguistics\tNNS\n",
            "27\tconcerned\tconcern\tVBN\n",
            "28\twith\twith\tIN\n",
            "29\tthe\tthe\tDT\n",
            "30\tinteractions\tinteraction\tNNS\n",
            "31\tbetween\tbetween\tIN\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install anago"
      ],
      "metadata": {
        "id": "MZ5IeL3Esci-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d841a2c6-020d-4b44-d629-5ab8ec08dd62"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting anago\n",
            "  Downloading anago-1.0.8-py3-none-any.whl (22 kB)\n",
            "Requirement already satisfied: h5py>=2.7.1 in /usr/local/lib/python3.7/dist-packages (from anago) (3.1.0)\n",
            "Requirement already satisfied: scikit-learn>=0.19.1 in /usr/local/lib/python3.7/dist-packages (from anago) (1.0.2)\n",
            "Requirement already satisfied: numpy>=1.14.3 in /usr/local/lib/python3.7/dist-packages (from anago) (1.21.6)\n",
            "Requirement already satisfied: Keras>=2.2.0 in /usr/local/lib/python3.7/dist-packages (from anago) (2.8.0)\n",
            "Requirement already satisfied: tensorflow>=1.8.0 in /usr/local/lib/python3.7/dist-packages (from anago) (2.8.2+zzzcolab20220527125636)\n",
            "Requirement already satisfied: requests>=2.18.4 in /usr/local/lib/python3.7/dist-packages (from anago) (2.23.0)\n",
            "Collecting seqeval>=0.0.3\n",
            "  Downloading seqeval-1.2.2.tar.gz (43 kB)\n",
            "\u001b[K     |████████████████████████████████| 43 kB 2.0 MB/s \n",
            "\u001b[?25hRequirement already satisfied: cached-property in /usr/local/lib/python3.7/dist-packages (from h5py>=2.7.1->anago) (1.5.2)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests>=2.18.4->anago) (2022.5.18.1)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests>=2.18.4->anago) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests>=2.18.4->anago) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests>=2.18.4->anago) (3.0.4)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn>=0.19.1->anago) (1.1.0)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn>=0.19.1->anago) (3.1.0)\n",
            "Requirement already satisfied: scipy>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn>=0.19.1->anago) (1.4.1)\n",
            "Requirement already satisfied: flatbuffers>=1.12 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=1.8.0->anago) (2.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=1.8.0->anago) (4.2.0)\n",
            "Requirement already satisfied: libclang>=9.0.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=1.8.0->anago) (14.0.1)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=1.8.0->anago) (0.26.0)\n",
            "Requirement already satisfied: absl-py>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=1.8.0->anago) (1.1.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=1.8.0->anago) (1.6.3)\n",
            "Requirement already satisfied: tensorflow-estimator<2.9,>=2.8 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=1.8.0->anago) (2.8.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=1.8.0->anago) (0.2.0)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=1.8.0->anago) (1.14.1)\n",
            "Requirement already satisfied: tensorboard<2.9,>=2.8 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=1.8.0->anago) (2.8.0)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=1.8.0->anago) (1.46.3)\n",
            "Requirement already satisfied: keras-preprocessing>=1.1.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=1.8.0->anago) (1.1.2)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from tensorflow>=1.8.0->anago) (57.4.0)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=1.8.0->anago) (3.3.0)\n",
            "Requirement already satisfied: protobuf<3.20,>=3.9.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=1.8.0->anago) (3.17.3)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=1.8.0->anago) (1.15.0)\n",
            "Requirement already satisfied: gast>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=1.8.0->anago) (0.5.3)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=1.8.0->anago) (1.1.0)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.7/dist-packages (from astunparse>=1.6.0->tensorflow>=1.8.0->anago) (0.37.1)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.9,>=2.8->tensorflow>=1.8.0->anago) (1.8.1)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.9,>=2.8->tensorflow>=1.8.0->anago) (0.4.6)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.9,>=2.8->tensorflow>=1.8.0->anago) (1.35.0)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.9,>=2.8->tensorflow>=1.8.0->anago) (1.0.1)\n",
            "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.9,>=2.8->tensorflow>=1.8.0->anago) (0.6.1)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.9,>=2.8->tensorflow>=1.8.0->anago) (3.3.7)\n",
            "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.9,>=2.8->tensorflow>=1.8.0->anago) (4.2.4)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.9,>=2.8->tensorflow>=1.8.0->anago) (0.2.8)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.9,>=2.8->tensorflow>=1.8.0->anago) (4.8)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.9,>=2.8->tensorflow>=1.8.0->anago) (1.3.1)\n",
            "Requirement already satisfied: importlib-metadata>=4.4 in /usr/local/lib/python3.7/dist-packages (from markdown>=2.6.8->tensorboard<2.9,>=2.8->tensorflow>=1.8.0->anago) (4.11.4)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard<2.9,>=2.8->tensorflow>=1.8.0->anago) (3.8.0)\n",
            "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.7/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.9,>=2.8->tensorflow>=1.8.0->anago) (0.4.8)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.9,>=2.8->tensorflow>=1.8.0->anago) (3.2.0)\n",
            "Building wheels for collected packages: seqeval\n",
            "  Building wheel for seqeval (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for seqeval: filename=seqeval-1.2.2-py3-none-any.whl size=16180 sha256=0c7e5424c5d56f6d225eccff87c0a821efa722ede4af9f074a3514cf1c55330c\n",
            "  Stored in directory: /root/.cache/pip/wheels/05/96/ee/7cac4e74f3b19e3158dce26a20a1c86b3533c43ec72a549fd7\n",
            "Successfully built seqeval\n",
            "Installing collected packages: seqeval, anago\n",
            "Successfully installed anago-1.0.8 seqeval-1.2.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "5J8SprzGFFUp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 新しいセクション"
      ],
      "metadata": {
        "id": "TJ9wlWYswCM5"
      }
    }
  ]
}