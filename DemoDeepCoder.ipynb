{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"self_compile.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyMkre8JSRb/0HZdvzXIp0n0"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["## 環境構築パート\n"],"metadata":{"id":"EAJVtXUjEhBl"}},{"cell_type":"code","source":["!pip uninstall -y tensorflow\n","!pip uninstall -y keras\n","!pip install tensorflow==2.2\n","!pip install keras=2.2"],"metadata":{"collapsed":true,"id":"HPRXeBToEmpr"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import tensorflow as tf\n","print(tf.__version__)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"kaVM_tvMFy29","executionInfo":{"status":"ok","timestamp":1661827890217,"user_tz":-540,"elapsed":2313,"user":{"displayName":"Lilith*","userId":"09940563216808006927"}},"outputId":"cf4d764d-7351-4e12-cac8-2596381c8be8"},"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["2.2.0\n"]}]},{"cell_type":"markdown","source":["#### ここでランタイムを再起動させる"],"metadata":{"id":"Ay7_l2AKFEqV"}},{"cell_type":"code","source":["!pip install tensorflow_addons==0.11.2\n","!pip install -U numpy==1.18.5\n","!pip install 'kashgari>=2.0.2'\n","!pip install sentencepiece"],"metadata":{"id":"cf8eCx4pExHr"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#### driveのマウント"],"metadata":{"id":"3C3cBAfPTQB-"}},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"yxpaeslbFByR","executionInfo":{"status":"ok","timestamp":1661827909710,"user_tz":-540,"elapsed":2227,"user":{"displayName":"Lilith*","userId":"09940563216808006927"}},"outputId":"d5a6e38e-806c-4eff-aa4c-96848bb82a9f"},"execution_count":3,"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"]}]},{"cell_type":"code","source":["%cd \"drive/My Drive/Colab Notebooks/self_compile\""],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"s6gYBRL-FDop","executionInfo":{"status":"ok","timestamp":1661827912942,"user_tz":-540,"elapsed":263,"user":{"displayName":"Lilith*","userId":"09940563216808006927"}},"outputId":"753c65ae-b618-4a52-98c0-9cb57fe1212b"},"execution_count":4,"outputs":[{"output_type":"stream","name":"stdout","text":["/content/drive/My Drive/Colab Notebooks/self_compile\n"]}]},{"cell_type":"code","source":["%ls"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"HiR3Bl82qcLs","executionInfo":{"status":"ok","timestamp":1661827930329,"user_tz":-540,"elapsed":281,"user":{"displayName":"Lilith*","userId":"09940563216808006927"}},"outputId":"a2a5a95d-2a78-4515-d6ab-f662bf4e5a58"},"execution_count":5,"outputs":[{"output_type":"stream","name":"stdout","text":["\u001b[0m\u001b[01;34minputs\u001b[0m/  self_compile.ipynb\n"]}]},{"cell_type":"markdown","source":["#### データの前処理"],"metadata":{"id":"y8930YuZTWRJ"}},{"cell_type":"code","source":["import codecs\n","import re\n","import requests\n","from bs4 import BeautifulSoup\n","\n","##### txtファイルの前処理(データ形式の統一->後々この処理いらない可能性が出てきたが一旦保留)\n","corpus_itest = []\n","with open('inputs/input_test.txt') as f:\n","    lines = f.read()\n","    for l in lines.split(\"\\n\"):\n","      l = l.lstrip()\n","      corpus_itest.append(l)\n","# ファイルの保存       \n","print(*corpus_itest, sep=\"\\n\", file=codecs.open(\"itest.txt\", \"w\", \"utf-8\"))\n","corpus_ctest = []\n","with open('inputs/collect_test.txt') as f:\n","    lines = f.read()\n","    for l in lines.split(\"\\n\"):\n","      l = l.lstrip()\n","      corpus_ctest.append(l)\n","# ファイルの保存       \n","print(*corpus_ctest, sep=\"\\n\", file=codecs.open(\"ctest.txt\", \"w\", \"utf-8\"))\n","\n","######## sentencepieceによるデータの単語分割\n","\"\"\"\n","  sentencepieceによる単語の分割は一般的な文章に対しては良いのかもしれないが今回のような\n","  プログラム体のテキストデータに対してはどうなのだろうというところがあるためここの分割方法に\n","  ついては別途要検討である可能性がある\n","\"\"\"\n","import sentencepiece as sp\n","# 学習の実行 - 非complete版\n","sp.SentencePieceTrainer.Train(\n","   input=\"itest.txt\",\n","   model_prefix=\"sentencepiece\",\n","   model_type=\"word\",\n","   vocab_size=82,\n","   add_dummy_prefix=\"false\", #先頭の_を入れるか入れないか => 先頭に来る単語と文中に来る単語で意味が異なる可能性によってtrue,　falseをきめる\n",")\n","#モデルの作成\n","sp_itest = sp.SentencePieceProcessor()\n","sp_itest.Load(\"sentencepiece.model\")\n","\n","# 学習の実行 - complete版\n","sp.SentencePieceTrainer.Train(\n","   input=\"ctest.txt\",\n","   model_prefix=\"sentencepiece\",\n","   model_type=\"word\",\n","   vocab_size=82,\n","   add_dummy_prefix=\"false\", #先頭の_を入れるか入れないか => 先頭に来る単語と文中に来る単語で意味が異なる可能性によってtrue,　falseをきめる\n",")\n","#モデルの作成\n","sp_ctest = sp.SentencePieceProcessor()\n","sp_ctest.Load(\"sentencepiece.model\")\n","\n","######### kashgariに埋め込むためのデータ整形\n","test_x = []\n","test_y = []\n","for i in range(len(corpus_ctest)):\n","  input_num = sp_itest.EncodeAsPieces(corpus_itest[i])\n","  collect_num = sp_ctest.EncodeAsPieces(corpus_ctest[i])\n","  if (len(input_num) != 0):\n","   #if (len(input_num) == len(collect_num)):\n","   test_x.append(input_num)\n","   tmp_y = []\n","   for j in range(len(input_num)):\n","     if (input_num[j] == collect_num[j]):\n","       tmp_y.append('C')\n","     else:\n","       tmp_y.append('X')\n","   test_y.append(tmp_y)"],"metadata":{"id":"rxJTje5LtPsd","executionInfo":{"status":"ok","timestamp":1661831091364,"user_tz":-540,"elapsed":253,"user":{"displayName":"Lilith*","userId":"09940563216808006927"}}},"execution_count":40,"outputs":[]},{"cell_type":"code","source":["import codecs\n","import re\n","import requests\n","from bs4 import BeautifulSoup\n","\n","corpus_itrain = []\n","with open('inputs/input_train.txt') as f:\n","    lines = f.read()\n","    \n","    for l in lines.split(\"\\n\"):\n","      l = l.lstrip()\n","      corpus_itrain.append(l)\n","\n","# ファイルの保存       \n","print(*corpus_itrain, sep=\"\\n\", file=codecs.open(\"itrain.txt\", \"w\", \"utf-8\"))\n","corpus_ctrain = []\n","with open('inputs/collect_train.txt') as f:\n","    lines = f.read()\n","    \n","    for l in lines.split(\"\\n\"):\n","      l = l.lstrip()\n","      corpus_ctrain.append(l)\n","\n","# ファイルの保存       \n","print(*corpus_ctrain, sep=\"\\n\", file=codecs.open(\"ctrain.txt\", \"w\", \"utf-8\"))\n","import sentencepiece as sp\n","\n","# 学習の実行 - 非complete版\n","sp.SentencePieceTrainer.Train(\n","   input=\"itrain.txt\",\n","   model_prefix=\"sentencepiece\",\n","   model_type=\"word\",\n","   vocab_size=82,\n","   add_dummy_prefix=\"false\", #先頭の_を入れるか入れないか => 先頭に来る単語と文中に来る単語で意味が異なる可能性によってtrue,　falseをきめる\n",")\n","#モデルの作成\n","sp_itrain = sp.SentencePieceProcessor()\n","sp_itrain.Load(\"sentencepiece.model\")\n","\n","# 学習の実行 - complete版\n","sp.SentencePieceTrainer.Train(\n","   input=\"ctrain.txt\",\n","   model_prefix=\"sentencepiece\",\n","   model_type=\"word\",\n","   vocab_size=82,\n","   add_dummy_prefix=\"false\", #先頭の_を入れるか入れないか => 先頭に来る単語と文中に来る単語で意味が異なる可能性によってtrue,　falseをきめる\n",")\n","#モデルの作成\n","sp_ctrain = sp.SentencePieceProcessor()\n","sp_ctrain.Load(\"sentencepiece.model\")\n","train_x = []\n","train_y = []\n","for i in range(len(corpus_ctrain)):\n","  input_num = sp_itrain.EncodeAsPieces(corpus_itrain[i])\n","  collect_num = sp_ctrain.EncodeAsPieces(corpus_ctrain[i])\n","  if (len(input_num) != 0):\n","   #if (len(input_num) == len(collect_num)):\n","   train_x.append(input_num)\n","   tmp_y = []\n","   for j in range(len(input_num)):\n","     if (input_num[j] == collect_num[j]):\n","       tmp_y.append('C')\n","     else:\n","       tmp_y.append('X')\n","   train_y.append(tmp_y)"],"metadata":{"id":"-nj5aY1hvQEs","executionInfo":{"status":"ok","timestamp":1661831096751,"user_tz":-540,"elapsed":2517,"user":{"displayName":"Lilith*","userId":"09940563216808006927"}}},"execution_count":41,"outputs":[]},{"cell_type":"code","source":["import codecs\n","import re\n","import requests\n","from bs4 import BeautifulSoup\n","\n","corpus_ivalid = []\n","with open('inputs/input_valid.txt') as f:\n","    lines = f.read()\n","    \n","    for l in lines.split(\"\\n\"):\n","      l = l.lstrip()\n","      corpus_ivalid.append(l)\n","\n","# ファイルの保存       \n","print(*corpus_ivalid, sep=\"\\n\", file=codecs.open(\"ivalid.txt\", \"w\", \"utf-8\"))\n","corpus_cvalid = []\n","with open('inputs/collect_valid.txt') as f:\n","    lines = f.read()\n","    \n","    for l in lines.split(\"\\n\"):\n","      l = l.lstrip()\n","      corpus_cvalid.append(l)\n","\n","# ファイルの保存       \n","print(*corpus_cvalid, sep=\"\\n\", file=codecs.open(\"cvalid.txt\", \"w\", \"utf-8\"))\n","import sentencepiece as sp\n","\n","# 学習の実行 - 非complete版\n","sp.SentencePieceTrainer.Train(\n","   input=\"ivalid.txt\",\n","   model_prefix=\"sentencepiece\",\n","   model_type=\"word\",\n","   vocab_size=82,\n","   add_dummy_prefix=\"false\", #先頭の_を入れるか入れないか => 先頭に来る単語と文中に来る単語で意味が異なる可能性によってtrue,　falseをきめる\n",")\n","#モデルの作成\n","sp_ivalid = sp.SentencePieceProcessor()\n","sp_ivalid.Load(\"sentencepiece.model\")\n","\n","# 学習の実行 - complete版\n","sp.SentencePieceTrainer.Train(\n","   input=\"cvalid.txt\",\n","   model_prefix=\"sentencepiece\",\n","   model_type=\"word\",\n","   vocab_size=82,\n","   add_dummy_prefix=\"false\", #先頭の_を入れるか入れないか => 先頭に来る単語と文中に来る単語で意味が異なる可能性によってtrue,　falseをきめる\n",")\n","#モデルの作成\n","sp_cvalid = sp.SentencePieceProcessor()\n","sp_cvalid.Load(\"sentencepiece.model\")\n","\n","valid_x = []\n","valid_y = []\n","for i in range(len(corpus_cvalid)):\n","  input_num = sp_ivalid.EncodeAsPieces(corpus_ivalid[i])\n","  collect_num = sp_cvalid.EncodeAsPieces(corpus_cvalid[i])\n","  if (len(input_num) != 0):\n","   #if (len(input_num) == len(collect_num)):\n","   valid_x.append(input_num)\n","   tmp_y = []\n","   for j in range(len(input_num)):\n","     if (input_num[j] == collect_num[j]):\n","       tmp_y.append('C')\n","     else:\n","       tmp_y.append('X')\n","   valid_y.append(tmp_y)"],"metadata":{"id":"htO9bMaUwc6y","executionInfo":{"status":"ok","timestamp":1661831104315,"user_tz":-540,"elapsed":1094,"user":{"displayName":"Lilith*","userId":"09940563216808006927"}}},"execution_count":42,"outputs":[]},{"cell_type":"markdown","source":["### 各入力データのフォーマット\n","- train_x  \n","学習データとして与える一文を組み合わせたlist  \n","自分の例であれば一行のコードをsentencepieceによって分割させたもの\n","- train_y  \n","train_xに紐づけるべきOかXが記されたラベルのlist  \n","listの長さはtrain_xと対応づけなければエラーが出るはずなので慎重に行う必要がある。\n","- valid_x  \n","よくわからないけど、trainとは別の補足をするための構造\n","- valid_y  \n","valid_xに対応するoxのlist\n","\n","- test_x  \n","評価用サンプルデータ、訓練したモデルからこのlistに対して評価を行い各単語に対してscoreを計算する\n","\n","- test_y  \n","test_xに対応するoxのlist\n","\n"],"metadata":{"id":"ll9eZ1I_VwEB"}},{"cell_type":"code","source":["import kashgari\n","from kashgari.tasks.labeling import BiLSTM_Model\n","\n","model = BiLSTM_Model()\n","\n","model.fit(train_x, train_y, valid_x, valid_y)\n","# Evaluate the model\n","\n","model.evaluate(test_x, test_y)\n","\n","# Model data will save to `saved_ner_model` folder\n","model.save('saved_classification_model')\n","\n","# Load saved model\n","loaded_model = BiLSTM_Model.load_model('saved_classification_model')\n","loaded_model.predict(test_x)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"AXCQ0LKPGuUb","executionInfo":{"status":"ok","timestamp":1661836536389,"user_tz":-540,"elapsed":335249,"user":{"displayName":"Lilith*","userId":"09940563216808006927"}},"outputId":"80cbff8c-8688-4f6f-98b5-8d938d47cd1d"},"execution_count":47,"outputs":[{"output_type":"stream","name":"stderr","text":["Preparing text vocab dict: 100%|██████████| 79216/79216 [00:00<00:00, 722518.66it/s]\n","Preparing text vocab dict: 100%|██████████| 35031/35031 [00:00<00:00, 695516.60it/s]\n","2022-08-30 05:10:01,334 [DEBUG] kashgari - --- Build vocab dict finished, Total: 10540 ---\n","2022-08-30 05:10:01,336 [DEBUG] kashgari - Top-10: ['[PAD]', '[UNK]', '[CLS]', '[SEP]', '▁=', '}', '▁{', '#define', '▁<<', 'int']\n","Preparing text vocab dict: 100%|██████████| 79216/79216 [00:00<00:00, 984672.24it/s]\n","Preparing text vocab dict: 100%|██████████| 35031/35031 [00:00<00:00, 932685.84it/s]\n","2022-08-30 05:10:01,472 [DEBUG] kashgari - --- Build vocab dict finished, Total: 3 ---\n","2022-08-30 05:10:01,474 [DEBUG] kashgari - Top-10: ['[PAD]', 'C', 'X']\n","Calculating sequence length: 100%|██████████| 79216/79216 [00:00<00:00, 1656013.80it/s]\n","Calculating sequence length: 100%|██████████| 35031/35031 [00:00<00:00, 1691443.97it/s]\n","2022-08-30 05:10:01,583 [DEBUG] kashgari - Calculated sequence length = 10\n","2022-08-30 05:10:02,836 [DEBUG] kashgari - fit input shape: (64, 10)\n","2022-08-30 05:10:02,839 [DEBUG] kashgari - fit input shape: (64, 10)\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 1/5\n","1237/1237 [==============================] - 66s 54ms/step - loss: 0.0143 - accuracy: 0.9882 - val_loss: 0.0018 - val_accuracy: 0.9999\n","Epoch 2/5\n","1237/1237 [==============================] - 64s 51ms/step - loss: 3.0353e-04 - accuracy: 1.0000 - val_loss: 0.0016 - val_accuracy: 0.9999\n","Epoch 3/5\n","1237/1237 [==============================] - 65s 52ms/step - loss: 2.1669e-04 - accuracy: 1.0000 - val_loss: 0.0015 - val_accuracy: 0.9999\n","Epoch 4/5\n","1237/1237 [==============================] - 65s 52ms/step - loss: 1.6750e-04 - accuracy: 1.0000 - val_loss: 0.0015 - val_accuracy: 0.9999\n","Epoch 5/5\n","1237/1237 [==============================] - 63s 51ms/step - loss: 1.8035e-04 - accuracy: 1.0000 - val_loss: 0.0015 - val_accuracy: 0.9999\n"]},{"output_type":"stream","name":"stderr","text":["2022-08-30 05:15:31,090 [WARNING] kashgari - Sequence length is None, will use the max length of the samples, which is 31\n","2022-08-30 05:15:31,096 [DEBUG] kashgari - predict seq_length: None, input: (171, 31)\n"]},{"output_type":"stream","name":"stdout","text":["6/6 [==============================] - 0s 16ms/step\n"]},{"output_type":"stream","name":"stderr","text":["2022-08-30 05:15:32,695 [DEBUG] kashgari - predict output: (171, 31)\n","2022-08-30 05:15:32,697 [DEBUG] kashgari - predict output argmax: [[0 1 1 ... 0 0 0]\n"," [0 1 1 ... 0 0 0]\n"," [0 1 1 ... 0 0 0]\n"," ...\n"," [0 1 1 ... 0 0 0]\n"," [0 1 1 ... 0 0 0]\n"," [0 1 0 ... 0 0 0]]\n"]},{"output_type":"stream","name":"stdout","text":["\n","           precision    recall  f1-score   support\n","\n","        C     0.7778    0.8160    0.7964       163\n","        X     0.0000    0.0000    0.0000        43\n","\n","micro avg     0.7778    0.6456    0.7056       206\n","macro avg     0.6154    0.6456    0.6302       206\n","\n"]},{"output_type":"stream","name":"stderr","text":["2022-08-30 05:15:33,197 [INFO] kashgari - model saved to /content/drive/MyDrive/Colab Notebooks/self_compile/saved_classification_model\n","2022-08-30 05:15:34,512 [WARNING] kashgari - Sequence length is None, will use the max length of the samples, which is 31\n","2022-08-30 05:15:34,516 [DEBUG] kashgari - predict seq_length: None, input: (171, 31)\n"]},{"output_type":"stream","name":"stdout","text":["6/6 [==============================] - 0s 18ms/step\n"]},{"output_type":"stream","name":"stderr","text":["2022-08-30 05:15:36,078 [DEBUG] kashgari - predict output: (171, 31)\n","2022-08-30 05:15:36,080 [DEBUG] kashgari - predict output argmax: [[0 1 1 ... 0 0 0]\n"," [0 1 1 ... 0 0 0]\n"," [0 1 1 ... 0 0 0]\n"," ...\n"," [0 1 1 ... 0 0 0]\n"," [0 1 1 ... 0 0 0]\n"," [0 1 0 ... 0 0 0]]\n"]},{"output_type":"execute_result","data":{"text/plain":["[['C', 'C'],\n"," ['C', 'C'],\n"," ['C', 'C'],\n"," ['C', 'C'],\n"," ['C'],\n"," ['C', 'C'],\n"," ['C', 'C'],\n"," ['C', 'C'],\n"," ['C', 'C'],\n"," ['C', 'C'],\n"," ['C', 'C'],\n"," ['C', 'C'],\n"," ['C'],\n"," ['C', 'C'],\n"," ['C', 'C'],\n"," ['C', 'C'],\n"," ['C', 'C'],\n"," ['C'],\n"," ['C'],\n"," ['C', 'C'],\n"," ['C', 'C'],\n"," ['C', 'C'],\n"," ['C', 'C', 'C', 'C'],\n"," ['C', 'C', 'C', 'C', 'C'],\n"," ['C', 'C'],\n"," ['C'],\n"," ['C', 'C'],\n"," ['C', 'C', 'C'],\n"," ['C', 'C', 'C'],\n"," ['C', 'C', 'C', 'C', 'C', 'C', 'C'],\n"," ['C', 'C'],\n"," ['C', 'C'],\n"," ['C', 'C', 'C', 'C'],\n"," ['C', 'C', 'C'],\n"," ['C', 'C', 'C', 'C', 'C', 'C', 'C', 'C'],\n"," ['C'],\n"," ['C'],\n"," ['C',\n","  'C',\n","  'C',\n","  'C',\n","  'C',\n","  'C',\n","  'C',\n","  'C',\n","  'C',\n","  'C',\n","  'C',\n","  'C',\n","  'C',\n","  'C',\n","  'C',\n","  'C',\n","  'C',\n","  'C'],\n"," ['C', 'C', 'C', 'C', 'C', 'C'],\n"," ['C',\n","  'C',\n","  'C',\n","  'C',\n","  'C',\n","  'C',\n","  'C',\n","  'C',\n","  'C',\n","  'C',\n","  'C',\n","  'C',\n","  'C',\n","  'C',\n","  'C',\n","  'C',\n","  'C',\n","  'C',\n","  'C'],\n"," ['C',\n","  'C',\n","  'C',\n","  'C',\n","  'C',\n","  'C',\n","  'C',\n","  'C',\n","  'C',\n","  'C',\n","  'C',\n","  'C',\n","  'C',\n","  'C',\n","  'C',\n","  'C',\n","  'C',\n","  'C',\n","  'C',\n","  'C',\n","  'C',\n","  'C',\n","  'C',\n","  'C',\n","  'C'],\n"," ['C', 'C', 'C', 'C', 'C', 'C', 'C', 'C', 'C', 'C', 'C', 'C', 'C'],\n"," ['C', 'C', 'C', 'C', 'C', 'C'],\n"," ['C',\n","  'C',\n","  'C',\n","  'C',\n","  'C',\n","  'C',\n","  'C',\n","  'C',\n","  'C',\n","  'C',\n","  'C',\n","  'C',\n","  'C',\n","  'C',\n","  'C',\n","  'C',\n","  'C',\n","  'C',\n","  'C'],\n"," ['C',\n","  'C',\n","  'C',\n","  'C',\n","  'C',\n","  'C',\n","  'C',\n","  'C',\n","  'C',\n","  'C',\n","  'C',\n","  'C',\n","  'C',\n","  'C',\n","  'C',\n","  'C',\n","  'C',\n","  'C',\n","  'C',\n","  'C',\n","  'C',\n","  'C',\n","  'C',\n","  'C',\n","  'C'],\n"," ['C', 'C', 'C', 'C', 'C', 'C', 'C', 'C', 'C', 'C', 'C', 'C', 'C'],\n"," ['C',\n","  'C',\n","  'C',\n","  'C',\n","  'C',\n","  'C',\n","  'C',\n","  'C',\n","  'C',\n","  'C',\n","  'C',\n","  'C',\n","  'C',\n","  'C',\n","  'C',\n","  'C',\n","  'C',\n","  'C',\n","  'C',\n","  'C',\n","  'C',\n","  'C',\n","  'C',\n","  'C',\n","  'C',\n","  'C',\n","  'C',\n","  'C'],\n"," ['C',\n","  'C',\n","  'C',\n","  'C',\n","  'C',\n","  'C',\n","  'C',\n","  'C',\n","  'C',\n","  'C',\n","  'C',\n","  'C',\n","  'C',\n","  'C',\n","  'C',\n","  'C',\n","  'C',\n","  'C',\n","  'C',\n","  'C',\n","  'C',\n","  'C',\n","  'C',\n","  'C',\n","  'C',\n","  'C',\n","  'C',\n","  'C',\n","  'C'],\n"," ['C',\n","  'C',\n","  'C',\n","  'C',\n","  'C',\n","  'C',\n","  'C',\n","  'C',\n","  'C',\n","  'C',\n","  'C',\n","  'C',\n","  'C',\n","  'C',\n","  'C',\n","  'C',\n","  'C',\n","  'C',\n","  'C',\n","  'C',\n","  'C'],\n"," ['C', 'C', 'C', 'C', 'C', 'C', 'C', 'C', 'C', 'C', 'C', 'C', 'C', 'C'],\n"," ['C', 'C', 'C', 'C', 'C', 'C', 'C', 'C', 'C', 'C', 'C', 'C', 'C', 'C'],\n"," ['C',\n","  'C',\n","  'C',\n","  'C',\n","  'C',\n","  'C',\n","  'C',\n","  'C',\n","  'C',\n","  'C',\n","  'C',\n","  'C',\n","  'C',\n","  'C',\n","  'C',\n","  'C',\n","  'C'],\n"," ['C',\n","  'C',\n","  'C',\n","  'C',\n","  'C',\n","  'C',\n","  'C',\n","  'C',\n","  'C',\n","  'C',\n","  'C',\n","  'C',\n","  'C',\n","  'C',\n","  'C',\n","  'C',\n","  'C'],\n"," ['C', 'C', 'C', 'C', 'C', 'C'],\n"," ['C', 'C', 'C', 'C', 'C', 'C', 'C', 'C', 'C', 'C', 'C', 'C'],\n"," ['C', 'C', 'C', 'C', 'C'],\n"," ['C'],\n"," ['C', 'C', 'C', 'C', 'C', 'C'],\n"," ['C'],\n"," ['C'],\n"," ['C', 'C'],\n"," ['C'],\n"," ['C'],\n"," ['C', 'C', 'C', 'C', 'C', 'C', 'C', 'C', 'C', 'C', 'C', 'C'],\n"," ['C', 'C', 'C', 'C', 'C', 'C', 'C', 'C', 'C', 'C', 'C', 'C', 'C', 'C'],\n"," ['C'],\n"," ['C'],\n"," ['C', 'C', 'C', 'C'],\n"," ['C', 'C', 'C', 'C', 'C', 'C'],\n"," ['C'],\n"," ['C', 'C', 'C', 'C', 'C', 'C', 'C', 'C', 'C'],\n"," ['C', 'C', 'C', 'C', 'C', 'C', 'C', 'C'],\n"," ['C', 'C', 'C', 'C', 'C'],\n"," ['C'],\n"," ['C', 'C', 'C', 'C', 'C'],\n"," ['C', 'C', 'C', 'C', 'C'],\n"," ['C', 'C', 'C', 'C', 'C', 'C', 'C', 'C'],\n"," ['C', 'C', 'C'],\n"," ['C'],\n"," ['C', 'C'],\n"," ['C'],\n"," ['C', 'C', 'C', 'C', 'C', 'C', 'C', 'C', 'C'],\n"," ['C', 'C', 'C', 'C', 'C', 'C'],\n"," ['C', 'C', 'C', 'C', 'C', 'C'],\n"," ['C'],\n"," ['C', 'C', 'C', 'C', 'C', 'C', 'C', 'C', 'C'],\n"," ['C', 'C', 'C', 'C', 'C', 'C', 'C', 'C'],\n"," ['C', 'C', 'C', 'C', 'C'],\n"," ['C', 'C', 'C', 'C', 'C', 'C'],\n"," ['C', 'C', 'C', 'C', 'C', 'C', 'C', 'C', 'C', 'C'],\n"," ['C', 'C', 'C', 'C', 'C', 'C', 'C', 'C', 'C', 'C', 'C', 'C'],\n"," ['C',\n","  'C',\n","  'C',\n","  'C',\n","  'C',\n","  'C',\n","  'C',\n","  'C',\n","  'C',\n","  'C',\n","  'C',\n","  'C',\n","  'C',\n","  'C',\n","  'C',\n","  'C',\n","  'C'],\n"," ['C',\n","  'C',\n","  'C',\n","  'C',\n","  'C',\n","  'C',\n","  'C',\n","  'C',\n","  'C',\n","  'C',\n","  'C',\n","  'C',\n","  'C',\n","  'C',\n","  'C',\n","  'C',\n","  'C',\n","  'C',\n","  'C',\n","  'C',\n","  'C'],\n"," ['C'],\n"," ['C', 'C'],\n"," ['C', 'C', 'C'],\n"," ['C', 'C', 'C', 'C', 'C', 'C'],\n"," ['C', 'C', 'C', 'C', 'C', 'C'],\n"," ['C', 'C', 'C', 'C', 'C'],\n"," ['C'],\n"," ['C'],\n"," ['C', 'C', 'C', 'C', 'C'],\n"," ['C', 'C', 'C'],\n"," ['C'],\n"," ['C', 'C', 'C', 'C', 'C', 'C', 'C', 'C', 'C', 'C', 'C', 'C'],\n"," ['C', 'C', 'C'],\n"," ['C'],\n"," ['C', 'C'],\n"," ['C', 'C', 'C'],\n"," ['C'],\n"," ['C'],\n"," ['C', 'C'],\n"," ['C'],\n"," ['C', 'C', 'C'],\n"," ['C'],\n"," ['C'],\n"," ['C', 'C', 'C', 'C', 'C'],\n"," ['C', 'C', 'C', 'C'],\n"," ['C', 'C', 'C', 'C'],\n"," ['C', 'C'],\n"," ['C', 'C', 'C'],\n"," ['C', 'C', 'C', 'C', 'C', 'C'],\n"," ['C', 'C', 'C', 'C'],\n"," ['C', 'C'],\n"," ['C', 'C'],\n"," ['C', 'C', 'C', 'C'],\n"," ['C', 'C', 'C', 'C'],\n"," ['C'],\n"," ['C'],\n"," ['C', 'C', 'C', 'C'],\n"," ['C', 'C', 'C'],\n"," ['C', 'C', 'C'],\n"," ['C'],\n"," ['C', 'C'],\n"," ['C', 'C', 'C'],\n"," ['C', 'C', 'C', 'C', 'C', 'C', 'C', 'C', 'C', 'C'],\n"," ['C', 'C', 'C', 'C'],\n"," ['C', 'C', 'C', 'C', 'C'],\n"," ['C'],\n"," ['C'],\n"," ['C', 'C', 'C'],\n"," ['C', 'C', 'C', 'C', 'C'],\n"," ['C', 'C', 'C', 'C', 'C', 'C', 'C', 'C', 'C', 'C', 'C'],\n"," ['C', 'C', 'C'],\n"," ['C'],\n"," ['C'],\n"," ['C'],\n"," ['C', 'C', 'C', 'C', 'C', 'C', 'C', 'C', 'C', 'C', 'C', 'C'],\n"," ['C', 'C', 'C', 'C', 'C', 'C', 'C', 'C', 'C', 'C', 'C'],\n"," ['C', 'C', 'C'],\n"," ['C'],\n"," ['C'],\n"," ['C'],\n"," ['C', 'C'],\n"," ['C', 'C', 'C', 'C', 'C', 'C', 'C', 'C', 'C', 'C', 'C'],\n"," ['C', 'C', 'C'],\n"," ['C'],\n"," ['C'],\n"," ['C'],\n"," ['C'],\n"," ['C'],\n"," ['C', 'C', 'C'],\n"," ['C'],\n"," ['C'],\n"," ['C'],\n"," ['C', 'C'],\n"," ['C', 'C', 'C'],\n"," ['C'],\n"," ['C', 'C', 'C'],\n"," ['C', 'C'],\n"," ['C']]"]},"metadata":{},"execution_count":47}]},{"cell_type":"markdown","source":["### extra\n","\n","```py\n","# テキストを語彙列に分割&&中身の確認\n","for i in corpus_itest:\n","  print(sp_itest.EncodeAsPieces(i))\n","  print(len(sp_itest.EncodeAsPieces(i)))\n","```\n","\n","### サンプル動作"],"metadata":{"id":"p8BFDBmhzWNB"}},{"cell_type":"code","source":["from kashgari.corpus import ChineseDailyNerCorpus\n","\n","ttrain_x, ttrain_y = ChineseDailyNerCorpus.load_data('train')\n","vvalid_x, vvalid_y = ChineseDailyNerCorpus.load_data('valid')\n","ttest_x, ttest_y = ChineseDailyNerCorpus.load_data('test')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"TR3WxQge5aTQ","executionInfo":{"status":"ok","timestamp":1661831867495,"user_tz":-540,"elapsed":1557,"user":{"displayName":"Lilith*","userId":"09940563216808006927"}},"outputId":"530906e9-e6f1-44d8-85c9-f02d0e80167f"},"execution_count":44,"outputs":[{"output_type":"stream","name":"stderr","text":["2022-08-30 03:57:47,047 [DEBUG] kashgari - loaded 20864 samples from /root/.kashgari/datasets/china-people-daily-ner-corpus/example.train. Sample:\n","x[0]: ['克', '罗', '地', '亚', '政', '府', '2', '4', '日', '正', '式', '向', '阿', '根', '廷', '政', '府', '提', '出', '引', '渡', '在', '阿', '侨', '居', '多', '年', '的', '前', '纳', '粹', '战', '犯', '沙', '基', '奇', '的', '要', '求', '。']\n","y[0]: ['B-LOC', 'I-LOC', 'I-LOC', 'I-LOC', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-LOC', 'I-LOC', 'I-LOC', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-LOC', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-PER', 'I-PER', 'I-PER', 'O', 'O', 'O', 'O']\n","2022-08-30 03:57:47,113 [DEBUG] kashgari - loaded 2318 samples from /root/.kashgari/datasets/china-people-daily-ner-corpus/example.dev. Sample:\n","x[0]: ['陈', '寅', '恪', '曾', '自', '称', '“', '思', '想', '囿', '于', '咸', '丰', '、', '同', '治', '之', '世', '，', '议', '论', '近', '乎', '曾', '湘', '乡', '（', '曾', '国', '藩', '）', '、', '张', '南', '皮', '（', '张', '之', '洞', '）', '之', '间', '”', '。']\n","y[0]: ['B-PER', 'I-PER', 'I-PER', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-PER', 'I-PER', 'O', 'B-PER', 'I-PER', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-PER', 'I-PER', 'I-PER', 'O', 'B-PER', 'I-PER', 'I-PER', 'O', 'O', 'B-PER', 'I-PER', 'I-PER', 'O', 'B-PER', 'I-PER', 'I-PER', 'O', 'O', 'O', 'O', 'O']\n","2022-08-30 03:57:47,234 [DEBUG] kashgari - loaded 4636 samples from /root/.kashgari/datasets/china-people-daily-ner-corpus/example.test. Sample:\n","x[0]: ['经', '过', '近', '期', '调', '整', '，', '新', '的', '汽', '车', '工', '业', '“', '春', '秋', '五', '霸', '”', '已', '经', '形', '成', '，', '它', '们', '是', '：', '美', '国', '通', '用', '、', '福', '特', '，', '日', '本', '丰', '田', '，', '德', '国', '大', '众', '和', '戴', '姆', '勒', '／', '克', '莱', '斯', '勒', '。']\n","y[0]: ['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-ORG', 'I-ORG', 'I-ORG', 'I-ORG', 'O', 'B-ORG', 'I-ORG', 'O', 'B-ORG', 'I-ORG', 'I-ORG', 'I-ORG', 'O', 'B-ORG', 'I-ORG', 'I-ORG', 'I-ORG', 'O', 'B-ORG', 'I-ORG', 'I-ORG', 'I-ORG', 'I-ORG', 'I-ORG', 'I-ORG', 'I-ORG', 'O']\n"]}]},{"cell_type":"code","source":["import kashgari\n","from kashgari.tasks.labeling import BiLSTM_Model\n","\n","model = BiLSTM_Model()\n","\n","model.fit(ttrain_x, ttrain_y, vvalid_x, vvalid_y)\n","# Evaluate the model\n","\n","model.evaluate(ttest_x, ttest_y)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"nUIViSZO5gkp","executionInfo":{"status":"ok","timestamp":1661832537532,"user_tz":-540,"elapsed":647900,"user":{"displayName":"Lilith*","userId":"09940563216808006927"}},"outputId":"c2266aa4-99f3-4cb2-e67a-1703adeb1550"},"execution_count":45,"outputs":[{"output_type":"stream","name":"stderr","text":["Preparing text vocab dict: 100%|██████████| 20864/20864 [00:00<00:00, 108706.81it/s]\n","Preparing text vocab dict: 100%|██████████| 2318/2318 [00:00<00:00, 102916.27it/s]\n","2022-08-30 03:58:09,839 [DEBUG] kashgari - --- Build vocab dict finished, Total: 3500 ---\n","2022-08-30 03:58:09,842 [DEBUG] kashgari - Top-10: ['[PAD]', '[UNK]', '[CLS]', '[SEP]', '，', '的', '。', '国', '一', '、']\n","Preparing text vocab dict: 100%|██████████| 20864/20864 [00:00<00:00, 145830.24it/s]\n","Preparing text vocab dict: 100%|██████████| 2318/2318 [00:00<00:00, 144626.87it/s]\n","2022-08-30 03:58:10,016 [DEBUG] kashgari - --- Build vocab dict finished, Total: 8 ---\n","2022-08-30 03:58:10,018 [DEBUG] kashgari - Top-10: ['[PAD]', 'O', 'I-ORG', 'I-LOC', 'B-LOC', 'I-PER', 'B-ORG', 'B-PER']\n","Calculating sequence length: 100%|██████████| 20864/20864 [00:00<00:00, 1098515.71it/s]\n","Calculating sequence length: 100%|██████████| 2318/2318 [00:00<00:00, 1261011.24it/s]\n","2022-08-30 03:58:10,075 [DEBUG] kashgari - Calculated sequence length = 97\n","2022-08-30 03:58:11,342 [DEBUG] kashgari - fit input shape: (64, 97)\n","2022-08-30 03:58:11,344 [DEBUG] kashgari - fit input shape: (64, 97)\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 1/5\n","326/326 [==============================] - 118s 361ms/step - loss: 0.2053 - accuracy: 0.5871 - val_loss: 0.0948 - val_accuracy: 0.4557\n","Epoch 2/5\n","326/326 [==============================] - 116s 355ms/step - loss: 0.0766 - accuracy: 0.4607 - val_loss: 0.0626 - val_accuracy: 0.4649\n","Epoch 3/5\n","326/326 [==============================] - 116s 356ms/step - loss: 0.0556 - accuracy: 0.4669 - val_loss: 0.0516 - val_accuracy: 0.4685\n","Epoch 4/5\n","326/326 [==============================] - 116s 356ms/step - loss: 0.0468 - accuracy: 0.4698 - val_loss: 0.0486 - val_accuracy: 0.4691\n","Epoch 5/5\n","326/326 [==============================] - 116s 357ms/step - loss: 0.0402 - accuracy: 0.4719 - val_loss: 0.0442 - val_accuracy: 0.4709\n"]},{"output_type":"stream","name":"stderr","text":["2022-08-30 04:07:59,755 [WARNING] kashgari - Sequence length is None, will use the max length of the samples, which is 579\n","2022-08-30 04:07:59,838 [DEBUG] kashgari - predict seq_length: None, input: (4636, 579)\n"]},{"output_type":"stream","name":"stdout","text":["145/145 [==============================] - 53s 364ms/step\n"]},{"output_type":"stream","name":"stderr","text":["2022-08-30 04:08:55,472 [DEBUG] kashgari - predict output: (4636, 579)\n","2022-08-30 04:08:55,473 [DEBUG] kashgari - predict output argmax: [[0 1 1 ... 1 1 1]\n"," [0 1 1 ... 1 1 1]\n"," [0 1 1 ... 1 1 1]\n"," ...\n"," [0 1 1 ... 1 1 1]\n"," [0 1 1 ... 1 1 1]\n"," [0 1 1 ... 1 1 1]]\n"]},{"output_type":"stream","name":"stdout","text":["\n","           precision    recall  f1-score   support\n","\n","      LOC     0.6537    0.7463    0.6970      3658\n","      ORG     0.5158    0.5835    0.5476      2185\n","      PER     0.7413    0.7918    0.7658      1864\n","\n","micro avg     0.6344    0.7112    0.6706      7707\n","macro avg     0.6358    0.7112    0.6712      7707\n","\n"]},{"output_type":"execute_result","data":{"text/plain":["{'detail': {'LOC': {'precision': 0.6537356321839081,\n","   'recall': 0.7463094587206124,\n","   'f1-score': 0.6969619606841971,\n","   'support': 3658},\n","  'ORG': {'precision': 0.5157766990291263,\n","   'recall': 0.5835240274599542,\n","   'f1-score': 0.5475628086751129,\n","   'support': 2185},\n","  'PER': {'precision': 0.7413360120542442,\n","   'recall': 0.7918454935622318,\n","   'f1-score': 0.7657587548638133,\n","   'support': 1864}},\n"," 'precision': 0.6358099593066676,\n"," 'recall': 0.7111716621253406,\n"," 'f1-score': 0.6712450899447338,\n"," 'support': 7707}"]},"metadata":{},"execution_count":45}]}]}